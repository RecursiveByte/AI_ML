#Version required

pip install tensorflow==2.19.0
pip install mediapipe==0.10.14
pip install protobuf==4.25.3

"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    HAND GESTURE RECOGNITION WITH DNN - COMPLETE MASTER COURSE
    From Zero to Hero: Understanding Every Single Step
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Author: Your AI Master Teacher
Purpose: Build a real hand gesture recognition system with deep understanding
"""

import cv2
import mediapipe as mp
import numpy as np
import os
import pickle
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

print("="*80)
print("SECTION 1: THE BIG PICTURE - What Are We Building?")
print("="*80)

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    THE COMPLETE PIPELINE                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                        â•‘
â•‘  Step 1: CAPTURE CAMERA â†’ Get video frames                            â•‘
â•‘           â†“                                                            â•‘
â•‘  Step 2: DETECT HAND â†’ Find hand in frame (MediaPipe)                 â•‘
â•‘           â†“                                                            â•‘
â•‘  Step 3: EXTRACT LANDMARKS â†’ Get 21 hand keypoints (x, y, z)          â•‘
â•‘           â†“                                                            â•‘
â•‘  Step 4: NORMALIZE DATA â†’ Make it independent of position/scale       â•‘
â•‘           â†“                                                            â•‘
â•‘  Step 5: TRAIN DNN â†’ Teach network to recognize gestures              â•‘
â•‘           â†“                                                            â•‘
â•‘  Step 6: PREDICT â†’ Real-time gesture recognition!                     â•‘
â•‘                                                                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

STORY TIME: Imagine teaching a child to recognize hand gestures
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

You show them different hand shapes (ğŸ‘, âœŒï¸, âœŠ) and say:
"This is thumbs up, this is peace sign, this is a fist"

The child learns by:
1. SEEING the hand (camera)
2. IDENTIFYING key points (thumb tip, finger tips - landmarks)
3. UNDERSTANDING relationships (thumb is UP, fingers are DOWN)
4. REMEMBERING patterns (brain learning - DNN training)
5. RECOGNIZING new gestures (prediction)

Our AI does EXACTLY the same thing!
""")

print("\n" + "="*80)
print("SECTION 2: UNDERSTANDING MEDIAPIPE - The Hand Detector")
print("="*80)

print("""
WHAT IS MEDIAPIPE?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MediaPipe is like having a super-smart assistant who can instantly find
and mark 21 key points on your hand.

Think of it like this:
â€¢ You show a photo to a friend
â€¢ Friend instantly points: "There's the thumb tip! There's the pinky!"
â€¢ Friend does this 30 times per second (real-time!)

THE 21 HAND LANDMARKS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       
         8  12  16  20        â† Finger tips
         â”‚  â”‚   â”‚   â”‚
         7  11  15  19        â† Finger joints (top)
         â”‚  â”‚   â”‚   â”‚
         6  10  14  18        â† Finger joints (middle)
         â”‚  â”‚   â”‚   â”‚
         5  9   13  17        â† Finger base
          â•² â”‚   â”‚  â•±
           4â”‚   â”‚ â•±           â† Thumb
            3  2â•±
             â•²â”‚â•±
              1               â† Wrist base
              â”‚
              0               â† Wrist center

Each landmark has 3 coordinates:
â€¢ X: Left-right position (0.0 to 1.0)
â€¢ Y: Up-down position (0.0 to 1.0)  
â€¢ Z: Depth/distance from camera (relative)

Total features: 21 landmarks Ã— 3 coordinates = 63 numbers!
""")

# Initialize MediaPipe
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=1,
    min_detection_confidence=0.7,
    min_tracking_confidence=0.7
)

print("\nâœ“ MediaPipe Hands initialized successfully!")
print("  â€¢ max_num_hands=1: We track ONE hand at a time")
print("  â€¢ min_detection_confidence=0.7: 70% sure it's a hand before detecting")
print("  â€¢ min_tracking_confidence=0.7: 70% sure we're still tracking same hand")

print("\n" + "="*80)
print("SECTION 3: DATA COLLECTION - Teaching the AI")
print("="*80)

print("""
THE TEACHING PROCESS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Just like teaching a child, we need to show MANY examples:

Gesture 0 (Fist):        Show 100 examples â†’ AI learns "fingers curled"
Gesture 1 (Thumbs up):   Show 100 examples â†’ AI learns "thumb up, fingers down"
Gesture 2 (Peace):       Show 100 examples â†’ AI learns "2 fingers up"
... and so on

WHY 100 EXAMPLES?
â€¢ More examples = Better learning
â€¢ Different angles, distances, lighting
â€¢ AI sees variations and learns the CORE pattern

DATA STRUCTURE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
After collection, we'll have:

X (features):           Shape: (1000, 63)
  [                     1000 examples, each with 63 numbers
    [x1, y1, z1, x2, y2, z2, ..., x21, y21, z21],  â† Example 1
    [x1, y1, z1, x2, y2, z2, ..., x21, y21, z21],  â† Example 2
    ...
  ]

y (labels):             Shape: (1000,)
  [0, 0, 0, ..., 1, 1, 1, ..., 2, 2, 2, ...]
   â””â”€100xâ”€â”˜    â””â”€100xâ”€â”˜    â””â”€100xâ”€â”˜
   Gesture 0   Gesture 1   Gesture 2
""")

class GestureDataCollector:
    """
    This class helps us collect hand gesture data
    Think of it as your data collection assistant!
    """
    
    def __init__(self):
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=1,
            min_detection_confidence=0.7
        )
        self.mp_drawing = mp.solutions.drawing_utils
        
    def extract_landmarks(self, image):
        """
        Extract 21 hand landmarks from an image
        
        INPUT: Image from camera (RGB format)
        OUTPUT: Array of 63 numbers [x1,y1,z1, x2,y2,z2, ..., x21,y21,z21]
        
        STORY: Like asking "Where are all the fingers and joints?"
        """
        # Convert BGR to RGB (OpenCV uses BGR, MediaPipe uses RGB)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Process the image
        results = self.hands.process(image_rgb)
        
        # If hand is detected
        if results.multi_hand_landmarks:
            hand_landmarks = results.multi_hand_landmarks[0]  # Get first hand
            
            # Extract all 21 landmarks (x, y, z)
            landmarks = []
            for landmark in hand_landmarks.landmark:
                landmarks.extend([landmark.x, landmark.y, landmark.z])
            
            return np.array(landmarks)
        
        return None
    
    def collect_data_for_gesture(self, gesture_name, gesture_label, num_samples=100):
        """
        Collect training data for ONE gesture
        
        PARAMETERS:
        â€¢ gesture_name: Name like "thumbs_up" (for display)
        â€¢ gesture_label: Number like 0, 1, 2 (for AI)
        â€¢ num_samples: How many examples to collect (default 100)
        
        STORY: Like taking 100 photos of someone making thumbs up
        """
        print(f"\n{'='*60}")
        print(f"Collecting data for: {gesture_name} (Label: {gesture_label})")
        print(f"{'='*60}")
        print(f"We'll collect {num_samples} examples")
        print(f"Press 's' to start collecting, 'q' to quit")
        
        cap = cv2.VideoCapture(0)
        collected_data = []
        labels = []
        count = 0
        collecting = False
        
        while count < num_samples:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame = cv2.flip(frame, 1)  # Mirror image
            
            # Try to extract landmarks
            landmarks = self.extract_landmarks(frame)
            
            # Draw status on frame
            status_color = (0, 255, 0) if collecting else (0, 0, 255)
            cv2.putText(frame, f"Gesture: {gesture_name}", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            cv2.putText(frame, f"Collected: {count}/{num_samples}", (10, 70),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, status_color, 2)
            cv2.putText(frame, f"Press 's' to start", (10, 110),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            # If collecting and hand detected
            if collecting and landmarks is not None:
                collected_data.append(landmarks)
                labels.append(gesture_label)
                count += 1
            
            cv2.imshow('Data Collection', frame)
            
            key = cv2.waitKey(1) & 0xFF
            if key == ord('s'):
                collecting = True
                print("Started collecting!")
            elif key == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()
        
        print(f"âœ“ Collected {count} examples for {gesture_name}")
        return np.array(collected_data), np.array(labels)

print("\nâœ“ Data Collector class ready!")
print("\nNOTE: To collect data, you would run:")
print("  collector = GestureDataCollector()")
print("  X, y = collector.collect_data_for_gesture('thumbs_up', 0, 100)")

print("\n" + "="*80)
print("SECTION 4: DATA NORMALIZATION - Making Data Universal")
print("="*80)

print("""
WHY NORMALIZE?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PROBLEM: Raw landmark coordinates depend on:
â€¢ Where your hand is in the frame (left/right/center)
â€¢ How close you are to camera (near/far)
â€¢ Camera resolution

Example:
Hand close to camera:    x=0.8, y=0.6
Same hand, far away:     x=0.5, y=0.5
^ Same gesture, different numbers! AI gets confused! ğŸ˜µ

SOLUTION: Normalize relative to WRIST (landmark 0)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

STORY: Imagine measuring your friend's height
â€¢ Don't say "5 feet from the ground" (depends where they stand)
â€¢ Say "5 feet tall" (relative to their feet) âœ“

Similarly:
â€¢ Don't say "thumb at position 0.8" (depends on hand location)
â€¢ Say "thumb 0.3 units above wrist" (relative to wrist) âœ“

MATHEMATICAL TRANSFORMATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Original: [x0, y0, z0, x1, y1, z1, ..., x21, y21, z21]
           â””â”€wristâ”€â”˜  â””â”€thumbâ”€â”˜

Normalized: All landmarks MINUS wrist position
x1_new = x1 - x0  (thumb X relative to wrist X)
y1_new = y1 - y0  (thumb Y relative to wrist Y)
z1_new = z1 - z0  (thumb Z relative to wrist Z)

Result: Gesture looks the same regardless of position! ğŸ¯
""")

def normalize_landmarks(landmarks):
    """
    Normalize landmarks relative to wrist (landmark 0)
    
    INPUT: [x0, y0, z0, x1, y1, z1, ..., x21, y21, z21]
    OUTPUT: Normalized values relative to wrist
    
    ANALOGY: Converting "position in room" to "position relative to person"
    """
    landmarks = landmarks.copy()
    
    # Extract wrist position (first landmark)
    wrist_x = landmarks[0]
    wrist_y = landmarks[1]
    wrist_z = landmarks[2]
    
    # Subtract wrist from all landmarks
    for i in range(0, len(landmarks), 3):
        landmarks[i] -= wrist_x      # Normalize X
        landmarks[i+1] -= wrist_y    # Normalize Y
        landmarks[i+2] -= wrist_z    # Normalize Z
    
    return landmarks

# Test normalization
test_landmarks = np.random.rand(63) * 0.5  # Random landmarks
normalized = normalize_landmarks(test_landmarks)

print("\nâœ“ Normalization function ready!")
print(f"\nExample transformation:")
print(f"Original wrist position: ({test_landmarks[0]:.3f}, {test_landmarks[1]:.3f}, {test_landmarks[2]:.3f})")
print(f"Normalized wrist position: ({normalized[0]:.3f}, {normalized[1]:.3f}, {normalized[2]:.3f})")
print(f"^ Notice: Wrist is now at (0, 0, 0) - our reference point!")

print("\n" + "="*80)
print("SECTION 5: BUILDING THE DNN - The Brain")
print("="*80)

print("""
THE NEURAL NETWORK ARCHITECTURE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Input Layer (63)         â† Hand landmark coordinates
     â†“
Dense Layer (128, relu)  â† First hidden layer: learns basic patterns
     â†“                     "thumb position", "finger angles"
Dropout (0.3)            â† Randomly drop 30% neurons (prevents overfitting)
     â†“
Dense Layer (64, relu)   â† Second hidden layer: combines patterns
     â†“                     "thumb up + fingers down = thumbs up"
Dropout (0.3)            â† More dropout
     â†“
Dense Layer (32, relu)   â† Third hidden layer: refines understanding
     â†“
Output Layer (10, softmax) â† 10 gestures with probabilities

LAYER-BY-LAYER EXPLANATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. INPUT LAYER (63 neurons)
   â€¢ Takes 63 numbers (21 landmarks Ã— 3 coordinates)
   â€¢ No processing, just passes data forward
   
2. DENSE(128, relu)
   â€¢ 128 neurons, each learning different pattern
   â€¢ ReLU activation: turns negatives to 0
   â€¢ Learns: "When thumb_x > 0.2 AND thumb_y < 0.3..."
   
3. DROPOUT(0.3)
   â€¢ During training, randomly ignores 30% of neurons
   â€¢ WHY? Prevents memorization, forces generalization
   â€¢ ANALOGY: Like studying with different friends each time
     (don't rely on one person's notes!)
   
4. DENSE(64, relu)
   â€¢ 64 neurons combining patterns from previous layer
   â€¢ Learns: "Pattern A + Pattern B = Specific gesture"
   
5. DENSE(32, relu)
   â€¢ 32 neurons for final refinement
   â€¢ Learns subtle differences between similar gestures
   
6. OUTPUT(10, softmax)
   â€¢ 10 neurons, one per gesture
   â€¢ Softmax: converts to probabilities that sum to 1
   â€¢ Output: [0.05, 0.02, 0.87, 0.01, ...]
              â””â”€â”€ 87% confident it's gesture 2!

WHY THIS ARCHITECTURE?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ 128 â†’ 64 â†’ 32: Funnel shape is common (starts wide, narrows down)
â€¢ Wide layers early: capture many patterns
â€¢ Narrow layers later: focus on what matters
â€¢ Dropout: prevents overfitting (memorizing training data)
""")

def create_gesture_model(num_classes=10):
    """
    Create the DNN model for gesture recognition
    
    PARAMETERS:
    â€¢ num_classes: Number of different gestures (default 10)
    
    RETURNS:
    â€¢ Compiled Keras model ready for training
    """
    model = Sequential([
        # Input layer implicitly defined by first Dense layer
        Dense(128, activation='relu', input_shape=(63,), name='hidden_layer_1'),
        Dropout(0.3, name='dropout_1'),
        
        Dense(64, activation='relu', name='hidden_layer_2'),
        Dropout(0.3, name='dropout_2'),
        
        Dense(32, activation='relu', name='hidden_layer_3'),
        
        Dense(num_classes, activation='softmax', name='output_layer')
    ])
    
    # Compile the model
    model.compile(
        optimizer='adam',      # Adam: Smart optimizer (adjusts learning rate)
        loss='categorical_crossentropy',  # For multi-class classification
        metrics=['accuracy']   # Track accuracy during training
    )
    
    return model

# Create and display model
model = create_gesture_model(num_classes=10)
print("\nâœ“ DNN Model created successfully!")
print("\nModel Architecture:")
model.summary()

print("\n" + "="*80)
print("SECTION 6: TRAINING THE MODEL - Learning Process")
print("="*80)

print("""
THE TRAINING PROCESS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Training is like teaching through repetition and feedback:

1. FORWARD PASS:
   â€¢ Show the network a hand gesture
   â€¢ Network makes a prediction
   â€¢ Example: Sees thumbs up, predicts [0.1, 0.6, 0.2, 0.1, ...]
   
2. CALCULATE LOSS:
   â€¢ Compare prediction with correct answer
   â€¢ Correct: [0, 1, 0, 0, ...]  (gesture 1)
   â€¢ Predicted: [0.1, 0.6, 0.2, 0.1, ...]
   â€¢ Loss: How wrong is this? Higher = worse
   
3. BACKWARD PASS (Backpropagation):
   â€¢ Calculate: "Which weights caused this error?"
   â€¢ Adjust weights slightly to reduce error
   â€¢ ANALOGY: "Oh, I was wrong because I gave too much importance
               to finger position. Let me adjust that."
   
4. REPEAT:
   â€¢ Do this for ALL training examples
   â€¢ One complete cycle = 1 EPOCH
   â€¢ Repeat for many epochs (usually 50-200)

HYPERPARAMETERS EXPLAINED:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â€¢ EPOCHS: How many times to show all data
  - Too few: Underfitting (didn't learn enough)
  - Too many: Overfitting (memorized training data)
  - Sweet spot: Usually 50-100

â€¢ BATCH SIZE: How many examples to show before updating weights
  - Small (16-32): More updates, slower, better for small datasets
  - Large (128-256): Fewer updates, faster, needs more data
  - We use 32: Good balance

â€¢ VALIDATION SPLIT: % of data to test on (not used for training)
  - We use 0.2 = 20% for testing
  - Helps detect overfitting

WHAT TO WATCH DURING TRAINING:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Training Accuracy:    How well it learns training data
Validation Accuracy:  How well it works on NEW data

GOOD SIGNS:
âœ“ Both accuracies increase
âœ“ Both are close (within 5-10%)
âœ“ Smooth curves

BAD SIGNS:
âœ— Training high (95%), Validation low (60%) â†’ OVERFITTING!
âœ— Both stuck at low values â†’ Model too simple or bad data
âœ— Wild fluctuations â†’ Learning rate too high
""")

def train_model(model, X_train, y_train, epochs=50, batch_size=32):
    """
    Train the gesture recognition model
    
    PARAMETERS:
    â€¢ model: The DNN model to train
    â€¢ X_train: Training data (shape: num_samples, 63)
    â€¢ y_train: Labels (shape: num_samples, num_classes)
    â€¢ epochs: Number of training cycles
    â€¢ batch_size: Examples per weight update
    
    RETURNS:
    â€¢ history: Training history (loss, accuracy over time)
    """
    print(f"\nStarting training...")
    print(f"  â€¢ Training samples: {X_train.shape[0]}")
    print(f"  â€¢ Features per sample: {X_train.shape[1]}")
    print(f"  â€¢ Epochs: {epochs}")
    print(f"  â€¢ Batch size: {batch_size}")
    
    history = model.fit(
        X_train, y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_split=0.2,  # 20% for validation
        verbose=1  # Show progress
    )
    
    return history

print("\nâœ“ Training function ready!")

print("\n" + "="*80)
print("SECTION 7: MAKING PREDICTIONS - Using the Trained Model")
print("="*80)

print("""
PREDICTION PROCESS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Once trained, prediction is simple and fast:

1. CAPTURE FRAME from camera
   â†“
2. DETECT HAND with MediaPipe
   â†“
3. EXTRACT 21 LANDMARKS (63 numbers)
   â†“
4. NORMALIZE landmarks (relative to wrist)
   â†“
5. FEED TO MODEL
   â†“
6. GET PROBABILITIES
   Output: [0.02, 0.05, 0.87, 0.01, 0.03, 0.01, 0.01, 0.00, 0.00, 0.00]
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    87% confident it's Gesture 2!
   â†“
7. PICK HIGHEST probability â†’ Final prediction!

MODEL OUTPUT INTERPRETATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Softmax output always sums to 1.0 (100%):
[0.02, 0.05, 0.87, 0.01, 0.03, 0.01, 0.01, 0.00, 0.00, 0.00]
 â”‚     â”‚     â”‚                                               â”‚
 2%    5%    87%  ... â† These are CONFIDENCES, not counts!  0%

CONFIDENCE THRESHOLD:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ If max confidence < 0.6 (60%) â†’ Don't predict (uncertain)
â€¢ If max confidence > 0.6 (60%) â†’ Show prediction (confident)

WHY? Prevents false predictions when:
â€¢ Hand partially visible
â€¢ Between two gestures
â€¢ Unusual hand position
""")

def predict_gesture(model, landmarks, gesture_names, threshold=0.6):
    """
    Predict gesture from hand landmarks
    
    PARAMETERS:
    â€¢ model: Trained DNN model
    â€¢ landmarks: Array of 63 hand coordinates
    â€¢ gesture_names: List of gesture names ['fist', 'thumbs_up', ...]
    â€¢ threshold: Minimum confidence to show prediction (0.0 to 1.0)
    
    RETURNS:
    â€¢ gesture_name: Predicted gesture name (or 'Unknown')
    â€¢ confidence: Confidence score (0.0 to 1.0)
    â€¢ all_probabilities: Array of all class probabilities
    """
    # Normalize landmarks
    normalized = normalize_landmarks(landmarks)
    
    # Reshape for model input (model expects batch dimension)
    input_data = normalized.reshape(1, -1)
    
    # Get prediction
    probabilities = model.predict(input_data, verbose=0)[0]
    
    # Get class with highest probability
    predicted_class = np.argmax(probabilities)
    confidence = probabilities[predicted_class]
    
    # Check threshold
    if confidence >= threshold:
        gesture_name = gesture_names[predicted_class]
    else:
        gesture_name = "Unknown"
    
    return gesture_name, confidence, probabilities

print("\nâœ“ Prediction function ready!")

print("\n" + "="*80)
print("SECTION 8: REAL-TIME GESTURE RECOGNITION")
print("="*80)

print("""
PUTTING IT ALL TOGETHER:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

The real-time recognition loop:

while camera_is_on:
    1. Capture frame from camera
    2. Detect hand with MediaPipe
    3. If hand found:
        a. Extract 21 landmarks (63 numbers)
        b. Normalize relative to wrist
        c. Feed to trained model
        d. Get prediction + confidence
        e. Display on screen
    4. Show frame with prediction overlay
    5. If 'q' pressed: quit

PERFORMANCE TIPS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Model prediction is VERY fast (~1-2ms)
â€¢ MediaPipe detection is fast (~10-15ms)
â€¢ Total: 30-60 FPS easily achievable!
â€¢ Bottleneck: Camera capture, not AI
""")

def real_time_recognition(model, gesture_names):
    """
    Run real-time gesture recognition
    
    PARAMETERS:
    â€¢ model: Trained DNN model
    â€¢ gesture_names: List of gesture names
    
    CONTROLS:
    â€¢ 'q': Quit
    """
    print("\nStarting real-time recognition...")
    print("Press 'q' to quit")
    
    cap = cv2.VideoCapture(0)
    mp_hands = mp.solutions.hands
    hands = mp_hands.Hands(
        static_image_mode=False,
        max_num_hands=1,
        min_detection_confidence=0.7
    )
    mp_drawing = mp.solutions.drawing_utils
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame = cv2.flip(frame, 1)
        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # Detect hand
        results = hands.process(image_rgb)
        
        if results.multi_hand_landmarks:
            # Draw hand landmarks
            for hand_landmarks in results.multi_hand_landmarks:
                mp_drawing.draw_landmarks(
                    frame, hand_landmarks, mp_hands.HAND_CONNECTIONS
                )
            
            # Extract and normalize landmarks
            hand_landmarks = results.multi_hand_landmarks[0]
            landmarks = []
            for landmark in hand_landmarks.landmark:
                landmarks.extend([landmark.x, landmark.y, landmark.z])
            landmarks = np.array(landmarks)
            
            # Predict gesture
            gesture, confidence, probs = predict_gesture(
                model, landmarks, gesture_names, threshold=0.6
            )
            
            # Display prediction
            if gesture != "Unknown":
                cv2.putText(frame, f"{gesture}: {confidence:.2f}", (10, 50),
                           cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)
            else:
                cv2.putText(frame, "Unknown gesture", (10, 50),
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
        
        cv2.imshow('Gesture Recognition', frame)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()
    hands.close()

print("\nâœ“ Real-time recognition function ready!")

print("\n" + "="*80)
print("SECTION 9: SAVING AND LOADING MODELS")
print("="*80)

print("""
WHY SAVE MODELS?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Training takes time (minutes to hours)
â€¢ Don't want to retrain every time!
â€¢ Save once, use forever

WHAT GETS SAVED?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Model architecture (layers, neurons)
â€¢ Trained weights (all the learned parameters)
â€¢ Optimizer state
â€¢ Everything needed to make predictions!

FILE FORMATS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ .h5: HDF5 format (older, widely supported)
â€¢ SavedModel: TensorFlow format (newer, recommended)
â€¢ .pkl: For data (not models)
""")

def save_model_and_config(model, gesture_names, filepath='gesture_model.h5'):
    """
    Save trained model and configuration
    
    PARAMETERS:
    â€¢ model: Trained Keras model
    â€¢ gesture_names: List of gesture names
    â€¢ filepath: Where to save model
    """
    # Save model
    model.save(filepath)
    print(f"âœ“ Model saved to {filepath}")
    
    # Save gesture names
    config_path = filepath.replace('.h5', '_config.pkl')
    with open(config_path, 'wb') as f:
        pickle.dump({'gesture_names': gesture_names}, f)
    print(f"âœ“ Configuration saved to {config_path}")

def load_model_and_config(filepath='gesture_model.h5'):
    """
    Load trained model and configuration
    
    PARAMETERS:
    â€¢ filepath: Path to saved model
    
    RETURNS:
    â€¢ model: Loaded Keras model
    â€¢ gesture_names: List of gesture names
    """
    # Load model
    model = load_model(filepath)
    print(f"âœ“ Model loaded from {filepath}")
    
    